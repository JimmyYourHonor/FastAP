{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, sys, numpy as np, argparse, imp, datetime, time, pickle as pkl, random, json\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import auxiliaries_nofaiss as aux\n",
    "\n",
    "import evaluate as eval\n",
    "\n",
    "from torchvision import transforms\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = os.getcwd()+'/Datasets/online_products'\n",
    "save_path = os.getcwd()+'/Training_Results/online_products'\n",
    "k_vals = [1,10,100,1000]\n",
    "batches_per_super_pair = 10\n",
    "sampling = 'None'\n",
    "pretrained = False\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50_mcn(nn.Module):\n",
    "    \"\"\"\n",
    "    class definition for the ResNet50 model imported from MatConvNet\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ResNet50_mcn, self).__init__()\n",
    "\n",
    "        \n",
    "        self.meta = {'mean': [0.485, 0.456, 0.406],\n",
    "                     'std': [0.229, 0.224, 0.225],\n",
    "                     'imageSize': [224, 224]}\n",
    "\n",
    "        self.features_0 = nn.Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.features_1 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_2 = nn.ReLU(inplace=True)\n",
    "        self.features_3 = nn.MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=1, dilation=1, ceil_mode=False)\n",
    "        self.features_4_0_conv1 = nn.Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_4_0_bn1 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_4_0_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_4_0_conv2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.features_4_0_bn2 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_4_0_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_4_0_conv3 = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_4_0_bn3 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_4_0_downsample_0 = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_4_0_downsample_1 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_4_0_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_4_1_conv1 = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_4_1_bn1 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_4_1_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_4_1_conv2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.features_4_1_bn2 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_4_1_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_4_1_conv3 = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_4_1_bn3 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_4_1_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_4_2_conv1 = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_4_2_bn1 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_4_2_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_4_2_conv2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.features_4_2_bn2 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_4_2_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_4_2_conv3 = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_4_2_bn3 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_4_2_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_5_0_conv1 = nn.Conv2d(256, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_5_0_bn1 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_5_0_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_5_0_conv2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        self.features_5_0_bn2 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_5_0_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_5_0_conv3 = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_5_0_bn3 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_5_0_downsample_0 = nn.Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.features_5_0_downsample_1 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_5_0_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_5_1_conv1 = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_5_1_bn1 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_5_1_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_5_1_conv2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.features_5_1_bn2 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_5_1_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_5_1_conv3 = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_5_1_bn3 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_5_1_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_5_2_conv1 = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_5_2_bn1 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_5_2_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_5_2_conv2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.features_5_2_bn2 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_5_2_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_5_2_conv3 = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_5_2_bn3 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_5_2_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_5_3_conv1 = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_5_3_bn1 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_5_3_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_5_3_conv2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.features_5_3_bn2 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_5_3_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_5_3_conv3 = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_5_3_bn3 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_5_3_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_6_0_conv1 = nn.Conv2d(512, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_6_0_bn1 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_0_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_6_0_conv2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        self.features_6_0_bn2 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_0_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_6_0_conv3 = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_6_0_bn3 = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_0_downsample_0 = nn.Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.features_6_0_downsample_1 = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_0_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_6_1_conv1 = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_6_1_bn1 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_1_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_6_1_conv2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.features_6_1_bn2 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_1_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_6_1_conv3 = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_6_1_bn3 = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_1_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_6_2_conv1 = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_6_2_bn1 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_2_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_6_2_conv2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.features_6_2_bn2 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_2_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_6_2_conv3 = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_6_2_bn3 = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_2_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_6_3_conv1 = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_6_3_bn1 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_3_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_6_3_conv2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.features_6_3_bn2 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_3_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_6_3_conv3 = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_6_3_bn3 = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_3_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_6_4_conv1 = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_6_4_bn1 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_4_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_6_4_conv2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.features_6_4_bn2 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_4_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_6_4_conv3 = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_6_4_bn3 = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_4_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_6_5_conv1 = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_6_5_bn1 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_5_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_6_5_conv2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.features_6_5_bn2 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_5_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_6_5_conv3 = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_6_5_bn3 = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_6_5_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_7_0_conv1 = nn.Conv2d(1024, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_7_0_bn1 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_7_0_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_7_0_conv2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        self.features_7_0_bn2 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_7_0_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_7_0_conv3 = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_7_0_bn3 = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_7_0_downsample_0 = nn.Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.features_7_0_downsample_1 = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_7_0_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_7_1_conv1 = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_7_1_bn1 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_7_1_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_7_1_conv2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.features_7_1_bn2 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_7_1_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_7_1_conv3 = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_7_1_bn3 = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_7_1_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_7_2_conv1 = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_7_2_bn1 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_7_2_relu1 = nn.ReLU(inplace=True)\n",
    "        self.features_7_2_conv2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.features_7_2_bn2 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_7_2_relu2 = nn.ReLU(inplace=True)\n",
    "        self.features_7_2_conv3 = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.features_7_2_bn3 = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.features_7_2_id_relu = nn.ReLU(inplace=True)\n",
    "        self.features_8 = nn.AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)\n",
    "        self.fc = nn.Linear(in_features=2048, out_features=512, bias=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        features_0 = self.features_0(data)\n",
    "        features_1 = self.features_1(features_0)\n",
    "        features_2 = self.features_2(features_1)\n",
    "        features_3 = self.features_3(features_2)\n",
    "        features_4_0_conv1 = self.features_4_0_conv1(features_3)\n",
    "        features_4_0_bn1 = self.features_4_0_bn1(features_4_0_conv1)\n",
    "        features_4_0_relu1 = self.features_4_0_relu1(features_4_0_bn1)\n",
    "        features_4_0_conv2 = self.features_4_0_conv2(features_4_0_relu1)\n",
    "        features_4_0_bn2 = self.features_4_0_bn2(features_4_0_conv2)\n",
    "        features_4_0_relu2 = self.features_4_0_relu2(features_4_0_bn2)\n",
    "        features_4_0_conv3 = self.features_4_0_conv3(features_4_0_relu2)\n",
    "        features_4_0_bn3 = self.features_4_0_bn3(features_4_0_conv3)\n",
    "        features_4_0_downsample_0 = self.features_4_0_downsample_0(features_3)\n",
    "        features_4_0_downsample_1 = self.features_4_0_downsample_1(features_4_0_downsample_0)\n",
    "        features_4_0_merge = torch.add(features_4_0_downsample_1, 1, features_4_0_bn3)\n",
    "        features_4_0_id_relu = self.features_4_0_id_relu(features_4_0_merge)\n",
    "        features_4_1_conv1 = self.features_4_1_conv1(features_4_0_id_relu)\n",
    "        features_4_1_bn1 = self.features_4_1_bn1(features_4_1_conv1)\n",
    "        features_4_1_relu1 = self.features_4_1_relu1(features_4_1_bn1)\n",
    "        features_4_1_conv2 = self.features_4_1_conv2(features_4_1_relu1)\n",
    "        features_4_1_bn2 = self.features_4_1_bn2(features_4_1_conv2)\n",
    "        features_4_1_relu2 = self.features_4_1_relu2(features_4_1_bn2)\n",
    "        features_4_1_conv3 = self.features_4_1_conv3(features_4_1_relu2)\n",
    "        features_4_1_bn3 = self.features_4_1_bn3(features_4_1_conv3)\n",
    "        features_4_1_merge = torch.add(features_4_0_id_relu, 1, features_4_1_bn3)\n",
    "        features_4_1_id_relu = self.features_4_1_id_relu(features_4_1_merge)\n",
    "        features_4_2_conv1 = self.features_4_2_conv1(features_4_1_id_relu)\n",
    "        features_4_2_bn1 = self.features_4_2_bn1(features_4_2_conv1)\n",
    "        features_4_2_relu1 = self.features_4_2_relu1(features_4_2_bn1)\n",
    "        features_4_2_conv2 = self.features_4_2_conv2(features_4_2_relu1)\n",
    "        features_4_2_bn2 = self.features_4_2_bn2(features_4_2_conv2)\n",
    "        features_4_2_relu2 = self.features_4_2_relu2(features_4_2_bn2)\n",
    "        features_4_2_conv3 = self.features_4_2_conv3(features_4_2_relu2)\n",
    "        features_4_2_bn3 = self.features_4_2_bn3(features_4_2_conv3)\n",
    "        features_4_2_merge = torch.add(features_4_1_id_relu, 1, features_4_2_bn3)\n",
    "        features_4_2_id_relu = self.features_4_2_id_relu(features_4_2_merge)\n",
    "        features_5_0_conv1 = self.features_5_0_conv1(features_4_2_id_relu)\n",
    "        features_5_0_bn1 = self.features_5_0_bn1(features_5_0_conv1)\n",
    "        features_5_0_relu1 = self.features_5_0_relu1(features_5_0_bn1)\n",
    "        features_5_0_conv2 = self.features_5_0_conv2(features_5_0_relu1)\n",
    "        features_5_0_bn2 = self.features_5_0_bn2(features_5_0_conv2)\n",
    "        features_5_0_relu2 = self.features_5_0_relu2(features_5_0_bn2)\n",
    "        features_5_0_conv3 = self.features_5_0_conv3(features_5_0_relu2)\n",
    "        features_5_0_bn3 = self.features_5_0_bn3(features_5_0_conv3)\n",
    "        features_5_0_downsample_0 = self.features_5_0_downsample_0(features_4_2_id_relu)\n",
    "        features_5_0_downsample_1 = self.features_5_0_downsample_1(features_5_0_downsample_0)\n",
    "        features_5_0_merge = torch.add(features_5_0_downsample_1, 1, features_5_0_bn3)\n",
    "        features_5_0_id_relu = self.features_5_0_id_relu(features_5_0_merge)\n",
    "        features_5_1_conv1 = self.features_5_1_conv1(features_5_0_id_relu)\n",
    "        features_5_1_bn1 = self.features_5_1_bn1(features_5_1_conv1)\n",
    "        features_5_1_relu1 = self.features_5_1_relu1(features_5_1_bn1)\n",
    "        features_5_1_conv2 = self.features_5_1_conv2(features_5_1_relu1)\n",
    "        features_5_1_bn2 = self.features_5_1_bn2(features_5_1_conv2)\n",
    "        features_5_1_relu2 = self.features_5_1_relu2(features_5_1_bn2)\n",
    "        features_5_1_conv3 = self.features_5_1_conv3(features_5_1_relu2)\n",
    "        features_5_1_bn3 = self.features_5_1_bn3(features_5_1_conv3)\n",
    "        features_5_1_merge = torch.add(features_5_0_id_relu, 1, features_5_1_bn3)\n",
    "        features_5_1_id_relu = self.features_5_1_id_relu(features_5_1_merge)\n",
    "        features_5_2_conv1 = self.features_5_2_conv1(features_5_1_id_relu)\n",
    "        features_5_2_bn1 = self.features_5_2_bn1(features_5_2_conv1)\n",
    "        features_5_2_relu1 = self.features_5_2_relu1(features_5_2_bn1)\n",
    "        features_5_2_conv2 = self.features_5_2_conv2(features_5_2_relu1)\n",
    "        features_5_2_bn2 = self.features_5_2_bn2(features_5_2_conv2)\n",
    "        features_5_2_relu2 = self.features_5_2_relu2(features_5_2_bn2)\n",
    "        features_5_2_conv3 = self.features_5_2_conv3(features_5_2_relu2)\n",
    "        features_5_2_bn3 = self.features_5_2_bn3(features_5_2_conv3)\n",
    "        features_5_2_merge = torch.add(features_5_1_id_relu, 1, features_5_2_bn3)\n",
    "        features_5_2_id_relu = self.features_5_2_id_relu(features_5_2_merge)\n",
    "        features_5_3_conv1 = self.features_5_3_conv1(features_5_2_id_relu)\n",
    "        features_5_3_bn1 = self.features_5_3_bn1(features_5_3_conv1)\n",
    "        features_5_3_relu1 = self.features_5_3_relu1(features_5_3_bn1)\n",
    "        features_5_3_conv2 = self.features_5_3_conv2(features_5_3_relu1)\n",
    "        features_5_3_bn2 = self.features_5_3_bn2(features_5_3_conv2)\n",
    "        features_5_3_relu2 = self.features_5_3_relu2(features_5_3_bn2)\n",
    "        features_5_3_conv3 = self.features_5_3_conv3(features_5_3_relu2)\n",
    "        features_5_3_bn3 = self.features_5_3_bn3(features_5_3_conv3)\n",
    "        features_5_3_merge = torch.add(features_5_2_id_relu, 1, features_5_3_bn3)\n",
    "        features_5_3_id_relu = self.features_5_3_id_relu(features_5_3_merge)\n",
    "        features_6_0_conv1 = self.features_6_0_conv1(features_5_3_id_relu)\n",
    "        features_6_0_bn1 = self.features_6_0_bn1(features_6_0_conv1)\n",
    "        features_6_0_relu1 = self.features_6_0_relu1(features_6_0_bn1)\n",
    "        features_6_0_conv2 = self.features_6_0_conv2(features_6_0_relu1)\n",
    "        features_6_0_bn2 = self.features_6_0_bn2(features_6_0_conv2)\n",
    "        features_6_0_relu2 = self.features_6_0_relu2(features_6_0_bn2)\n",
    "        features_6_0_conv3 = self.features_6_0_conv3(features_6_0_relu2)\n",
    "        features_6_0_bn3 = self.features_6_0_bn3(features_6_0_conv3)\n",
    "        features_6_0_downsample_0 = self.features_6_0_downsample_0(features_5_3_id_relu)\n",
    "        features_6_0_downsample_1 = self.features_6_0_downsample_1(features_6_0_downsample_0)\n",
    "        features_6_0_merge = torch.add(features_6_0_downsample_1, 1, features_6_0_bn3)\n",
    "        features_6_0_id_relu = self.features_6_0_id_relu(features_6_0_merge)\n",
    "        features_6_1_conv1 = self.features_6_1_conv1(features_6_0_id_relu)\n",
    "        features_6_1_bn1 = self.features_6_1_bn1(features_6_1_conv1)\n",
    "        features_6_1_relu1 = self.features_6_1_relu1(features_6_1_bn1)\n",
    "        features_6_1_conv2 = self.features_6_1_conv2(features_6_1_relu1)\n",
    "        features_6_1_bn2 = self.features_6_1_bn2(features_6_1_conv2)\n",
    "        features_6_1_relu2 = self.features_6_1_relu2(features_6_1_bn2)\n",
    "        features_6_1_conv3 = self.features_6_1_conv3(features_6_1_relu2)\n",
    "        features_6_1_bn3 = self.features_6_1_bn3(features_6_1_conv3)\n",
    "        features_6_1_merge = torch.add(features_6_0_id_relu, 1, features_6_1_bn3)\n",
    "        features_6_1_id_relu = self.features_6_1_id_relu(features_6_1_merge)\n",
    "        features_6_2_conv1 = self.features_6_2_conv1(features_6_1_id_relu)\n",
    "        features_6_2_bn1 = self.features_6_2_bn1(features_6_2_conv1)\n",
    "        features_6_2_relu1 = self.features_6_2_relu1(features_6_2_bn1)\n",
    "        features_6_2_conv2 = self.features_6_2_conv2(features_6_2_relu1)\n",
    "        features_6_2_bn2 = self.features_6_2_bn2(features_6_2_conv2)\n",
    "        features_6_2_relu2 = self.features_6_2_relu2(features_6_2_bn2)\n",
    "        features_6_2_conv3 = self.features_6_2_conv3(features_6_2_relu2)\n",
    "        features_6_2_bn3 = self.features_6_2_bn3(features_6_2_conv3)\n",
    "        features_6_2_merge = torch.add(features_6_1_id_relu, 1, features_6_2_bn3)\n",
    "        features_6_2_id_relu = self.features_6_2_id_relu(features_6_2_merge)\n",
    "        features_6_3_conv1 = self.features_6_3_conv1(features_6_2_id_relu)\n",
    "        features_6_3_bn1 = self.features_6_3_bn1(features_6_3_conv1)\n",
    "        features_6_3_relu1 = self.features_6_3_relu1(features_6_3_bn1)\n",
    "        features_6_3_conv2 = self.features_6_3_conv2(features_6_3_relu1)\n",
    "        features_6_3_bn2 = self.features_6_3_bn2(features_6_3_conv2)\n",
    "        features_6_3_relu2 = self.features_6_3_relu2(features_6_3_bn2)\n",
    "        features_6_3_conv3 = self.features_6_3_conv3(features_6_3_relu2)\n",
    "        features_6_3_bn3 = self.features_6_3_bn3(features_6_3_conv3)\n",
    "        features_6_3_merge = torch.add(features_6_2_id_relu, 1, features_6_3_bn3)\n",
    "        features_6_3_id_relu = self.features_6_3_id_relu(features_6_3_merge)\n",
    "        features_6_4_conv1 = self.features_6_4_conv1(features_6_3_id_relu)\n",
    "        features_6_4_bn1 = self.features_6_4_bn1(features_6_4_conv1)\n",
    "        features_6_4_relu1 = self.features_6_4_relu1(features_6_4_bn1)\n",
    "        features_6_4_conv2 = self.features_6_4_conv2(features_6_4_relu1)\n",
    "        features_6_4_bn2 = self.features_6_4_bn2(features_6_4_conv2)\n",
    "        features_6_4_relu2 = self.features_6_4_relu2(features_6_4_bn2)\n",
    "        features_6_4_conv3 = self.features_6_4_conv3(features_6_4_relu2)\n",
    "        features_6_4_bn3 = self.features_6_4_bn3(features_6_4_conv3)\n",
    "        features_6_4_merge = torch.add(features_6_3_id_relu, 1, features_6_4_bn3)\n",
    "        features_6_4_id_relu = self.features_6_4_id_relu(features_6_4_merge)\n",
    "        features_6_5_conv1 = self.features_6_5_conv1(features_6_4_id_relu)\n",
    "        features_6_5_bn1 = self.features_6_5_bn1(features_6_5_conv1)\n",
    "        features_6_5_relu1 = self.features_6_5_relu1(features_6_5_bn1)\n",
    "        features_6_5_conv2 = self.features_6_5_conv2(features_6_5_relu1)\n",
    "        features_6_5_bn2 = self.features_6_5_bn2(features_6_5_conv2)\n",
    "        features_6_5_relu2 = self.features_6_5_relu2(features_6_5_bn2)\n",
    "        features_6_5_conv3 = self.features_6_5_conv3(features_6_5_relu2)\n",
    "        features_6_5_bn3 = self.features_6_5_bn3(features_6_5_conv3)\n",
    "        features_6_5_merge = torch.add(features_6_4_id_relu, 1, features_6_5_bn3)\n",
    "        features_6_5_id_relu = self.features_6_5_id_relu(features_6_5_merge)\n",
    "        features_7_0_conv1 = self.features_7_0_conv1(features_6_5_id_relu)\n",
    "        features_7_0_bn1 = self.features_7_0_bn1(features_7_0_conv1)\n",
    "        features_7_0_relu1 = self.features_7_0_relu1(features_7_0_bn1)\n",
    "        features_7_0_conv2 = self.features_7_0_conv2(features_7_0_relu1)\n",
    "        features_7_0_bn2 = self.features_7_0_bn2(features_7_0_conv2)\n",
    "        features_7_0_relu2 = self.features_7_0_relu2(features_7_0_bn2)\n",
    "        features_7_0_conv3 = self.features_7_0_conv3(features_7_0_relu2)\n",
    "        features_7_0_bn3 = self.features_7_0_bn3(features_7_0_conv3)\n",
    "        features_7_0_downsample_0 = self.features_7_0_downsample_0(features_6_5_id_relu)\n",
    "        features_7_0_downsample_1 = self.features_7_0_downsample_1(features_7_0_downsample_0)\n",
    "        features_7_0_merge = torch.add(features_7_0_downsample_1, 1, features_7_0_bn3)\n",
    "        features_7_0_id_relu = self.features_7_0_id_relu(features_7_0_merge)\n",
    "        features_7_1_conv1 = self.features_7_1_conv1(features_7_0_id_relu)\n",
    "        features_7_1_bn1 = self.features_7_1_bn1(features_7_1_conv1)\n",
    "        features_7_1_relu1 = self.features_7_1_relu1(features_7_1_bn1)\n",
    "        features_7_1_conv2 = self.features_7_1_conv2(features_7_1_relu1)\n",
    "        features_7_1_bn2 = self.features_7_1_bn2(features_7_1_conv2)\n",
    "        features_7_1_relu2 = self.features_7_1_relu2(features_7_1_bn2)\n",
    "        features_7_1_conv3 = self.features_7_1_conv3(features_7_1_relu2)\n",
    "        features_7_1_bn3 = self.features_7_1_bn3(features_7_1_conv3)\n",
    "        features_7_1_merge = torch.add(features_7_0_id_relu, 1, features_7_1_bn3)\n",
    "        features_7_1_id_relu = self.features_7_1_id_relu(features_7_1_merge)\n",
    "        features_7_2_conv1 = self.features_7_2_conv1(features_7_1_id_relu)\n",
    "        features_7_2_bn1 = self.features_7_2_bn1(features_7_2_conv1)\n",
    "        features_7_2_relu1 = self.features_7_2_relu1(features_7_2_bn1)\n",
    "        features_7_2_conv2 = self.features_7_2_conv2(features_7_2_relu1)\n",
    "        features_7_2_bn2 = self.features_7_2_bn2(features_7_2_conv2)\n",
    "        features_7_2_relu2 = self.features_7_2_relu2(features_7_2_bn2)\n",
    "        features_7_2_conv3 = self.features_7_2_conv3(features_7_2_relu2)\n",
    "        features_7_2_bn3 = self.features_7_2_bn3(features_7_2_conv3)\n",
    "        features_7_2_merge = torch.add(features_7_1_id_relu, 1, features_7_2_bn3)\n",
    "        features_7_2_id_relu = self.features_7_2_id_relu(features_7_2_merge)\n",
    "        features_8 = self.features_8(features_7_2_id_relu)\n",
    "        classifier_flatten = features_8.view(features_8.size(0), -1)\n",
    "        logits = self.fc(classifier_flatten)\n",
    "\n",
    "        #No Normalization is used if N-Pair Loss is the target criterion.\n",
    "        return torch.nn.functional.normalize(logits, dim=-1)\n",
    "\n",
    "    def load_pth(self, weights_path):\n",
    "        if weights_path:\n",
    "            state_dict = torch.load(weights_path)\n",
    "            self.load_state_dict(state_dict)\n",
    "\n",
    "    def to_optim(self, opt):\n",
    "        return [{'params':self.parameters(),'lr':opt.lr,'weight_decay':opt.decay}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50_mcn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet50_mcn(\n",
       "  (features_0): Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (features_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_2): ReLU(inplace=True)\n",
       "  (features_3): MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=1, dilation=1, ceil_mode=False)\n",
       "  (features_4_0_conv1): Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_4_0_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_4_0_relu1): ReLU(inplace=True)\n",
       "  (features_4_0_conv2): Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (features_4_0_bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_4_0_relu2): ReLU(inplace=True)\n",
       "  (features_4_0_conv3): Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_4_0_bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_4_0_downsample_0): Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_4_0_downsample_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_4_0_id_relu): ReLU(inplace=True)\n",
       "  (features_4_1_conv1): Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_4_1_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_4_1_relu1): ReLU(inplace=True)\n",
       "  (features_4_1_conv2): Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (features_4_1_bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_4_1_relu2): ReLU(inplace=True)\n",
       "  (features_4_1_conv3): Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_4_1_bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_4_1_id_relu): ReLU(inplace=True)\n",
       "  (features_4_2_conv1): Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_4_2_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_4_2_relu1): ReLU(inplace=True)\n",
       "  (features_4_2_conv2): Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (features_4_2_bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_4_2_relu2): ReLU(inplace=True)\n",
       "  (features_4_2_conv3): Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_4_2_bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_4_2_id_relu): ReLU(inplace=True)\n",
       "  (features_5_0_conv1): Conv2d(256, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_5_0_bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_5_0_relu1): ReLU(inplace=True)\n",
       "  (features_5_0_conv2): Conv2d(128, 128, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (features_5_0_bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_5_0_relu2): ReLU(inplace=True)\n",
       "  (features_5_0_conv3): Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_5_0_bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_5_0_downsample_0): Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
       "  (features_5_0_downsample_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_5_0_id_relu): ReLU(inplace=True)\n",
       "  (features_5_1_conv1): Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_5_1_bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_5_1_relu1): ReLU(inplace=True)\n",
       "  (features_5_1_conv2): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (features_5_1_bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_5_1_relu2): ReLU(inplace=True)\n",
       "  (features_5_1_conv3): Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_5_1_bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_5_1_id_relu): ReLU(inplace=True)\n",
       "  (features_5_2_conv1): Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_5_2_bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_5_2_relu1): ReLU(inplace=True)\n",
       "  (features_5_2_conv2): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (features_5_2_bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_5_2_relu2): ReLU(inplace=True)\n",
       "  (features_5_2_conv3): Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_5_2_bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_5_2_id_relu): ReLU(inplace=True)\n",
       "  (features_5_3_conv1): Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_5_3_bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_5_3_relu1): ReLU(inplace=True)\n",
       "  (features_5_3_conv2): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (features_5_3_bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_5_3_relu2): ReLU(inplace=True)\n",
       "  (features_5_3_conv3): Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_5_3_bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_5_3_id_relu): ReLU(inplace=True)\n",
       "  (features_6_0_conv1): Conv2d(512, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_6_0_bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_0_relu1): ReLU(inplace=True)\n",
       "  (features_6_0_conv2): Conv2d(256, 256, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (features_6_0_bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_0_relu2): ReLU(inplace=True)\n",
       "  (features_6_0_conv3): Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_6_0_bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_0_downsample_0): Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
       "  (features_6_0_downsample_1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_0_id_relu): ReLU(inplace=True)\n",
       "  (features_6_1_conv1): Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_6_1_bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_1_relu1): ReLU(inplace=True)\n",
       "  (features_6_1_conv2): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (features_6_1_bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_1_relu2): ReLU(inplace=True)\n",
       "  (features_6_1_conv3): Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_6_1_bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_1_id_relu): ReLU(inplace=True)\n",
       "  (features_6_2_conv1): Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_6_2_bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_2_relu1): ReLU(inplace=True)\n",
       "  (features_6_2_conv2): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (features_6_2_bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_2_relu2): ReLU(inplace=True)\n",
       "  (features_6_2_conv3): Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_6_2_bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_2_id_relu): ReLU(inplace=True)\n",
       "  (features_6_3_conv1): Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_6_3_bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_3_relu1): ReLU(inplace=True)\n",
       "  (features_6_3_conv2): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (features_6_3_bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_3_relu2): ReLU(inplace=True)\n",
       "  (features_6_3_conv3): Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_6_3_bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_3_id_relu): ReLU(inplace=True)\n",
       "  (features_6_4_conv1): Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_6_4_bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_4_relu1): ReLU(inplace=True)\n",
       "  (features_6_4_conv2): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (features_6_4_bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_4_relu2): ReLU(inplace=True)\n",
       "  (features_6_4_conv3): Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_6_4_bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_4_id_relu): ReLU(inplace=True)\n",
       "  (features_6_5_conv1): Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_6_5_bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_5_relu1): ReLU(inplace=True)\n",
       "  (features_6_5_conv2): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (features_6_5_bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_5_relu2): ReLU(inplace=True)\n",
       "  (features_6_5_conv3): Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_6_5_bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_6_5_id_relu): ReLU(inplace=True)\n",
       "  (features_7_0_conv1): Conv2d(1024, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_7_0_bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_7_0_relu1): ReLU(inplace=True)\n",
       "  (features_7_0_conv2): Conv2d(512, 512, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (features_7_0_bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_7_0_relu2): ReLU(inplace=True)\n",
       "  (features_7_0_conv3): Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_7_0_bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_7_0_downsample_0): Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
       "  (features_7_0_downsample_1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_7_0_id_relu): ReLU(inplace=True)\n",
       "  (features_7_1_conv1): Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_7_1_bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_7_1_relu1): ReLU(inplace=True)\n",
       "  (features_7_1_conv2): Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (features_7_1_bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_7_1_relu2): ReLU(inplace=True)\n",
       "  (features_7_1_conv3): Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_7_1_bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_7_1_id_relu): ReLU(inplace=True)\n",
       "  (features_7_2_conv1): Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_7_2_bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_7_2_relu1): ReLU(inplace=True)\n",
       "  (features_7_2_conv2): Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (features_7_2_bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_7_2_relu2): ReLU(inplace=True)\n",
       "  (features_7_2_conv3): Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (features_7_2_bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (features_7_2_id_relu): ReLU(inplace=True)\n",
       "  (features_8): AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)\n",
       "  (fc): Linear(in_features=2048, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_optim = [{'params': model.parameters(), 'lr': 0.00001, 'weight_decay': 0.0004}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_dataloaders():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset: string, name of dataset for which the dataloaders should be returned.\n",
    "        opt:     argparse.Namespace, contains all training-specific parameters.\n",
    "    Returns:\n",
    "        dataloaders: dict of dataloaders for training, testing and evaluation on training.\n",
    "    \"\"\"\n",
    "    #Dataset selection\n",
    "    \n",
    "    \n",
    "    datasets = give_OnlineProducts_datasets(os.getcwd()+'/Datasets/online_products')\n",
    "    \n",
    "\n",
    "    #Move datasets to dataloaders.\n",
    "    dataloaders = {}\n",
    "    for key, dataset in datasets.items():\n",
    "        if isinstance(dataset, SuperLabelTrainDataset) and key == 'training':\n",
    "            # important: use a SequentialSampler\n",
    "            # see reasoning in class definition of SuperLabelTrainDataset\n",
    "            dataloaders[key] = torch.utils.data.DataLoader(dataset, batch_size=112, \n",
    "                    num_workers=8, sampler=torch.utils.data.SequentialSampler(dataset), \n",
    "                    pin_memory=True, drop_last=False)\n",
    "        else:\n",
    "            is_val = dataset.is_validation\n",
    "            dataloaders[key] = torch.utils.data.DataLoader(dataset, batch_size=112, \n",
    "                    num_workers=8, shuffle=not is_val, pin_memory=True, drop_last=not is_val)\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_OnlineProducts_datasets(source_path):\n",
    "    \"\"\"\n",
    "    This function generates a training, testing and evaluation dataloader for Metric Learning on the Online-Products dataset.\n",
    "    For Metric Learning, training and test sets are provided by given text-files, Ebay_train.txt & Ebay_test.txt.\n",
    "    So no random shuffling of classes.\n",
    "\n",
    "    Args:\n",
    "        opt: argparse.Namespace, contains all traininig-specific parameters.\n",
    "    Returns:\n",
    "        dict of PyTorch datasets for training, testing and evaluation.\n",
    "    \"\"\"\n",
    "    image_sourcepath  = source_path+'/images'\n",
    "    #Load text-files containing classes and imagepaths.\n",
    "    training_files = pd.read_table(source_path+'/Info_Files/Ebay_train.txt', header=0, delimiter=' ')\n",
    "    test_files     = pd.read_table(source_path+'/Info_Files/Ebay_test.txt', header=0, delimiter=' ')\n",
    "\n",
    "    #Generate Conversion dict.\n",
    "    conversion = {}\n",
    "    for class_id, path in zip(training_files['class_id'],training_files['path']):\n",
    "        conversion[class_id] = path.split('/')[0]\n",
    "    for class_id, path in zip(test_files['class_id'],test_files['path']):\n",
    "        conversion[class_id] = path.split('/')[0]\n",
    "\n",
    "    #Generate image_dicts of shape {class_idx:[list of paths to images belong to this class] ...}\n",
    "    train_image_dict, val_image_dict  = {},{}\n",
    "    for key, img_path in zip(training_files['class_id'],training_files['path']):\n",
    "        key = key-1\n",
    "        if not key in train_image_dict.keys():\n",
    "            train_image_dict[key] = []\n",
    "        train_image_dict[key].append(image_sourcepath+'/'+img_path)\n",
    "\n",
    "    for key, img_path in zip(test_files['class_id'],test_files['path']):\n",
    "        key = key-1\n",
    "        if not key in val_image_dict.keys():\n",
    "            val_image_dict[key] = []\n",
    "        val_image_dict[key].append(image_sourcepath+'/'+img_path)\n",
    "\n",
    "    ### Uncomment this if super-labels should be used to generate resp.datasets\n",
    "    # super_conversion = {}\n",
    "    # for super_class_id, path in zip(training_files['super_class_id'],training_files['path']):\n",
    "    #     conversion[super_class_id] = path.split('/')[0]\n",
    "    # for key, img_path in zip(training_files['super_class_id'],training_files['path']):\n",
    "    #     key = key-1\n",
    "    #     if not key in super_train_image_dict.keys():\n",
    "    #         super_train_image_dict[key] = []\n",
    "    #     super_train_image_dict[key].append(image_sourcepath+'/'+img_path)\n",
    "    # super_train_dataset = BaseTripletDataset(super_train_image_dict, opt, is_validation=True)\n",
    "    # super_train_dataset.conversion = super_conversion\n",
    "\n",
    "    \n",
    "    super_dict = {}\n",
    "    for cid, scid, path in zip(training_files['class_id'], training_files['super_class_id'], training_files['path']):\n",
    "        cid  = cid - 1\n",
    "        scid = scid - 1\n",
    "        if not scid in super_dict.keys():\n",
    "            super_dict[scid] = {}\n",
    "        if not cid in super_dict[scid].keys():\n",
    "            super_dict[scid][cid] = []\n",
    "        super_dict[scid][cid].append(image_sourcepath+'/'+path)\n",
    "        train_dataset = SuperLabelTrainDataset(super_dict)\n",
    "    \n",
    "\n",
    "    val_dataset   = BaseTripletDataset(val_image_dict, is_validation=True)\n",
    "    eval_dataset  = BaseTripletDataset(train_image_dict, is_validation=True)\n",
    "\n",
    "    train_dataset.conversion       = conversion\n",
    "    val_dataset.conversion         = conversion\n",
    "    eval_dataset.conversion        = conversion\n",
    "\n",
    "    return {'training':train_dataset, 'testing':val_dataset, 'evaluation':eval_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class BaseTripletDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class to provide (augmented) correctly prepared training samples corresponding to standard DML literature.\n",
    "    This includes normalizing to ImageNet-standards, and Random & Resized cropping of shapes 224 for ResNet50 and 227 for\n",
    "    GoogLeNet during Training. During validation, only resizing to 256 or center cropping to 224/227 is performed.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dict, samples_per_class=8, is_validation=False):\n",
    "        \"\"\"\n",
    "        Dataset Init-Function.\n",
    "\n",
    "        Args:\n",
    "            image_dict:         dict, Dictionary of shape {class_idx:[list of paths to images belong to this class] ...} providing all the training paths and classes.\n",
    "            opt:                argparse.Namespace, contains all training-specific parameters.\n",
    "            samples_per_class:  Number of samples to draw from one class before moving to the next when filling the batch.\n",
    "            is_validation:      If is true, dataset properties for validation/testing are used instead of ones for training.\n",
    "        Returns:\n",
    "            Nothing!\n",
    "        \"\"\"\n",
    "        #Define length of dataset\n",
    "        self.n_files     = np.sum([len(image_dict[key]) for key in image_dict.keys()])\n",
    "\n",
    "        self.is_validation = is_validation\n",
    "\n",
    "        \n",
    "        self.image_dict  = image_dict\n",
    "\n",
    "        self.avail_classes    = sorted(list(self.image_dict.keys()))\n",
    "\n",
    "        #Convert image dictionary from classname:content to class_idx:content, because the initial indices are not necessarily from 0 - <n_classes>.\n",
    "        self.image_dict    = {i:self.image_dict[key] for i,key in enumerate(self.avail_classes)}\n",
    "        self.avail_classes = sorted(list(self.image_dict.keys()))\n",
    "\n",
    "        #Init. properties that are used when filling up batches.\n",
    "        if not self.is_validation:\n",
    "            self.samples_per_class = samples_per_class\n",
    "            #Select current class to sample images from up to <samples_per_class>\n",
    "            self.current_class   = np.random.randint(len(self.avail_classes))\n",
    "            self.classes_visited = [self.current_class, self.current_class]\n",
    "            self.n_samples_drawn = 0\n",
    "\n",
    "        #Data augmentation/processing methods.\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "        transf_list = []\n",
    "        if not self.is_validation:\n",
    "            transf_list.extend([transforms.RandomResizedCrop(size=224)])\n",
    "        else:\n",
    "            transf_list.extend([transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224)])\n",
    "\n",
    "        transf_list.extend([transforms.ToTensor(), normalize])\n",
    "        self.transform = transforms.Compose(transf_list)\n",
    "\n",
    "        #Convert Image-Dict to list of (image_path, image_class). Allows for easier direct sampling.\n",
    "        self.image_list = [[(x,key) for x in self.image_dict[key]] for key in self.image_dict.keys()]\n",
    "        self.image_list = [x for y in self.image_list for x in y]\n",
    "\n",
    "        #Flag that denotes if dataset is called for the first time.\n",
    "        self.is_init = True\n",
    "\n",
    "\n",
    "    def ensure_3dim(self, img):\n",
    "        \"\"\"\n",
    "        Function that ensures that the input img is three-dimensional.\n",
    "\n",
    "        Args:\n",
    "            img: PIL.Image, image which is to be checked for three-dimensionality (i.e. if some images are black-and-white in an otherwise coloured dataset).\n",
    "        Returns:\n",
    "            Checked PIL.Image img.\n",
    "        \"\"\"\n",
    "        if len(img.size)==2:\n",
    "            img = img.convert('RGB')\n",
    "        return img\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: Sample idx for training sample\n",
    "        Returns:\n",
    "            tuple of form (sample_class, torch.Tensor() of input image)\n",
    "        \"\"\"\n",
    "        if self.is_init:\n",
    "            self.current_class = self.avail_classes[idx%len(self.avail_classes)]\n",
    "            self.is_init = False\n",
    "\n",
    "        if not self.is_validation:\n",
    "            if self.samples_per_class==1:\n",
    "                return self.image_list[idx][-1], self.transform(self.ensure_3dim(Image.open(self.image_list[idx][0])))\n",
    "\n",
    "            if (self.samples_per_class == 0 and self.n_samples_drawn == len(self.image_dict[self.current_class])\n",
    "                or self.n_samples_drawn == self.samples_per_class):\n",
    "                #Once enough samples per class have been drawn, we choose another class to draw samples from.\n",
    "                #Note that we ensure with self.classes_visited that no class is chosen if it had been chosen\n",
    "                #previously or one before that.\n",
    "                #NOTE: if self.samples_per_class is 0, then use all the images from current_class\n",
    "                counter = copy.deepcopy(self.avail_classes)\n",
    "                for prev_class in self.classes_visited:\n",
    "                    if prev_class in counter: counter.remove(prev_class)\n",
    "\n",
    "                self.current_class   = counter[idx%len(counter)]\n",
    "                self.classes_visited = self.classes_visited[1:]+[self.current_class]\n",
    "                self.n_samples_drawn = 0\n",
    "\n",
    "            class_sample_idx = idx%len(self.image_dict[self.current_class])\n",
    "            self.n_samples_drawn += 1\n",
    "\n",
    "            out_img = self.transform(self.ensure_3dim(Image.open(self.image_dict[self.current_class][class_sample_idx])))\n",
    "            return self.current_class,out_img\n",
    "        else:\n",
    "            return self.image_list[idx][-1], self.transform(self.ensure_3dim(Image.open(self.image_list[idx][0])))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperLabelTrainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class to provide (augmented) correctly prepared training samples, utilizing\n",
    "    super-label information to construct the batches.\n",
    "\n",
    "    Each batch takes a pair of super-labels (s1,s2). Then, for each s{i}, sample half the batch\n",
    "    from classes belonging to it.\n",
    "\n",
    "    NOTE: \n",
    "        SuperLabelTrainDataset implements a custom reshuffle(), so it's important that DataLoader \n",
    "        does NOT do further randomization. This means it should use a SequentialSampler.\n",
    "    TODO:\n",
    "        support samples_per_class\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dict, super_pairs=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dict: two-level dict, `super_dict[super_class_id][class_id]` gives the list of \n",
    "                        image paths having the same super-label and class label\n",
    "        \"\"\"\n",
    "        self.batch_size = 112\n",
    "        self.batches_per_super_pair = 5\n",
    "        self.samples_per_class = 0\n",
    "\n",
    "        # checks\n",
    "        assert self.batch_size % 2 == 0, \"opt.bs should be an even number\"\n",
    "        self.half_bs = self.batch_size // 2\n",
    "        if self.samples_per_class > 0:\n",
    "            assert self.half_bs % self.samples_per_class == 0, \"opt.bs not a multiple of opt.samples_per_class\"\n",
    "\n",
    "        # provide avail_classes\n",
    "        self.avail_classes = []\n",
    "        for sid in image_dict.keys():\n",
    "            self.avail_classes += list(image_dict[sid].keys())\n",
    "\n",
    "        # Data augmentation/processing methods.\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "        transf_list = []\n",
    "        transf_list.extend([\n",
    "            transforms.RandomResizedCrop(size=224)])\n",
    "        transf_list.extend([transforms.ToTensor(), normalize])\n",
    "        self.transform = transforms.Compose(transf_list)\n",
    "\n",
    "        # for each super-label, store a list of lists:\n",
    "        # super_image_lists[super0]: [\n",
    "        #       [(class0, image0), (class0, image1), ...], \n",
    "        #       [(class1, image0), (class1, image1), ...], \n",
    "        #     ], \n",
    "        # ...\n",
    "        self.super_image_lists = {}\n",
    "        for sid in image_dict.keys():\n",
    "            self.super_image_lists[sid] = []\n",
    "            for cid in image_dict[sid].keys():\n",
    "                cur_cid_list = list(itertools.product([cid], image_dict[sid][cid]))\n",
    "                self.super_image_lists[sid].append(cur_cid_list)\n",
    "\n",
    "        if super_pairs is None:\n",
    "            self.super_pairs = list(itertools.combinations(image_dict.keys(), 2))\n",
    "        else:\n",
    "            self.super_pairs = super_pairs  # allow super_pairs to be supplied\n",
    "\n",
    "        self.reshuffle()\n",
    "\n",
    "\n",
    "    def ensure_3dim(self, img):\n",
    "        if len(img.size) == 2:\n",
    "            img = img.convert('RGB')\n",
    "        return img\n",
    "\n",
    "\n",
    "    def reshuffle(self):\n",
    "        # for each super-label, concat all images into a long list:\n",
    "        # super_images[super0]: [\n",
    "        #       (class0_in_super0, image0), (class0_in_super0, image1), ...\n",
    "        #       (class1_in_super0, image0), (class1_in_super0, image1), ...\n",
    "        #       ...\n",
    "        #     ] \n",
    "        # ...\n",
    "        super_images, num_images, cur_pos = {}, {}, {}\n",
    "\n",
    "        for sid in self.super_image_lists.keys():\n",
    "            all_imgs_in_super = self.super_image_lists[sid]\n",
    "\n",
    "            if self.samples_per_class > 0:\n",
    "                chunks_list = []\n",
    "                for cls_imgs in all_imgs_in_super:\n",
    "                    random.shuffle(cls_imgs)\n",
    "                    num = len(cls_imgs)\n",
    "                    # take chunks of size `samples_per_class` and append to chunks_list\n",
    "                    for c in range(math.ceil(num / self.samples_per_class)):\n",
    "                        inds = [i % num for i in range(c*self.samples_per_class, (c+1)*self.samples_per_class)]\n",
    "                        chunks_list.append([cls_imgs[i] for i in inds])\n",
    "                # concat a \"list of lists\" into a long list\n",
    "                random.shuffle(chunks_list)\n",
    "                super_images[sid] = list(itertools.chain.from_iterable(chunks_list))\n",
    "            else:\n",
    "                for cls_imgs in all_imgs_in_super:\n",
    "                    random.shuffle(cls_imgs)  # shuffle images in each class\n",
    "                # concat a \"list of lists\" into a long list\n",
    "                random.shuffle(all_imgs_in_super)\n",
    "                super_images[sid] = list(itertools.chain.from_iterable(all_imgs_in_super))\n",
    "\n",
    "            num_images[sid] = len(super_images[sid])\n",
    "            cur_pos[sid] = 0\n",
    "\n",
    "        # pre-compute all the batches\n",
    "        # batches = [\n",
    "        #   [(cid,img), (cid,img), ...],   # batch No.0\n",
    "        #   [(cid,img), (cid,img), ...],   # batch No.1\n",
    "        #   ...\n",
    "        # ]\n",
    "        self.batches = []\n",
    "\n",
    "        # for each pair of super-labels, e.g. (bicycle, chair)\n",
    "        for pair in self.super_pairs:\n",
    "            s0, s1 = pair\n",
    "            # sample `batches_per_super_pair` batches\n",
    "            for b in range(self.batches_per_super_pair):\n",
    "                # get half of the batch from each super-label\n",
    "                ind0 = [(cur_pos[s0]+i) % num_images[s0] for i in range(self.half_bs)]\n",
    "                ind1 = [(cur_pos[s1]+i) % num_images[s1] for i in range(self.half_bs)]\n",
    "                cur_batch = [super_images[s0][i] for i in ind0] + [super_images[s1][i] for i in ind1]\n",
    "\n",
    "                # move pointers and append to list\n",
    "                cur_pos[s0] = (ind0[-1] + 1) % num_images[s0]\n",
    "                cur_pos[s1] = (ind1[-1] + 1) % num_images[s1]\n",
    "                self.batches.append(cur_batch)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # we use SequentialSampler together with SuperLabelTrainDataset,\n",
    "        # so idx==0 indicates the start of a new epoch\n",
    "        if idx == 0:\n",
    "            self.reshuffle()\n",
    "\n",
    "        batch_idx    = idx // self.batch_size  # global batch index\n",
    "        batch_offset = idx % self.batch_size   # offset from start of this batch\n",
    "        batch_item   = self.batches[batch_idx][batch_offset]\n",
    "\n",
    "        cls = batch_item[0]\n",
    "        img = Image.open(batch_item[1])\n",
    "        return cls, self.transform(self.ensure_3dim(img))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches) * self.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders      = give_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes  = len(dataloaders['training'].dataset.avail_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11318"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# FastAP loss proposed in CVPR'19 paper \"Deep Metric Learning to Rank\"\n",
    "from FastAP_loss import FastAPLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "histbins = 10\n",
    "loss_params  = {'num_bins':histbins}\n",
    "criterion    = FastAPLoss(**loss_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'params': <generator object Module.parameters at 0x7fd16b9f34d0>,\n",
       "   'lr': 1e-05,\n",
       "   'weight_decay': 0.0004}],\n",
       " FastAPLoss())"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_optim, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer    = torch.optim.Adam(to_optim)\n",
    "scheduler    = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 50], gamma=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_dataloader, model, optimizer, criterion, epoch):\n",
    "    \"\"\"\n",
    "    This function is called every epoch to perform training of the network over one full\n",
    "    (randomized) iteration of the dataset.\n",
    "\n",
    "    Args:\n",
    "        train_dataloader: torch.utils.data.DataLoader, returns (augmented) training data.\n",
    "        model:            Network to train.\n",
    "        optimizer:        Optimizer to use for training.\n",
    "        criterion:        criterion to use during training.\n",
    "        opt:              argparse.Namespace, Contains all relevant parameters.\n",
    "        epoch:            int, Current epoch.\n",
    "\n",
    "    Returns:\n",
    "        Nothing!\n",
    "    \"\"\"\n",
    "    loss_collect = []\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    data_iterator = tqdm(train_dataloader, desc='Epoch {} Training...'.format(epoch))\n",
    "    for i,(class_labels, input) in enumerate(data_iterator):\n",
    "        #Compute embeddings for input batch.\n",
    "        features  = model(input.to(device))\n",
    "        #Compute loss.\n",
    "        loss      = criterion(features, class_labels)\n",
    "\n",
    "        #Ensure gradients are set to zero at beginning\n",
    "        optimizer.zero_grad()\n",
    "        #Compute gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "\n",
    "        #Update weights using comp. gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        #Store loss per iteration.\n",
    "        loss_collect.append(loss.item())\n",
    "        if i==len(train_dataloader)-1: data_iterator.set_description('Epoch (Train) {0}: Mean Loss [{1:.4f}]'.format(epoch, np.mean(loss_collect)))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (Train) 1: Mean Loss [0.8028]: 100%|| 330/330 [10:48:44<00:00, 117.95s/it]\n"
     ]
    }
   ],
   "source": [
    "_ = model.train()\n",
    "train_one_epoch(dataloaders['training'], model, optimizer, criterion, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_params = {'dataloader':dataloaders['testing'], 'model':model, 'epoch':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import squareform, pdist, cdist\n",
    "def eval_metrics_one_dataset(model, test_dataloader, device, k_vals):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics on test-dataset, e.g. NMI, F1 and Recall @ k.\n",
    "\n",
    "    Args:\n",
    "        model:              PyTorch network, network to compute evaluation metrics for.\n",
    "        test_dataloader:    PyTorch Dataloader, dataloader for test dataset, should have no shuffling and correct processing.\n",
    "        device:             torch.device, Device to run inference on.\n",
    "        k_vals:             list of int, Recall values to compute\n",
    "        \n",
    "    Returns:\n",
    "        F1 score (float), NMI score (float), recall_at_k (list of float), data embedding (np.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    _ = model.eval()\n",
    "    n_classes = len(test_dataloader.dataset.avail_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ### For all test images, extract features\n",
    "        target_labels, feature_coll = [],[]\n",
    "        final_iter = tqdm(test_dataloader, desc='Computing Evaluation Metrics...')\n",
    "        image_paths= [x[0] for x in test_dataloader.dataset.image_list]\n",
    "        for idx,inp in enumerate(final_iter):\n",
    "            input_img,target = inp[-1], inp[0]\n",
    "            target_labels.extend(target.numpy().tolist())\n",
    "            out = model(input_img.to(device))\n",
    "            feature_coll.extend(out.cpu().detach().numpy().tolist())\n",
    "\n",
    "        target_labels = np.hstack(target_labels).reshape(-1,1)\n",
    "        feature_coll  = np.vstack(feature_coll).astype('float32')\n",
    "\n",
    "\n",
    "        ### Set Faiss CPU Cluster index\n",
    "        # cpu_cluster_index = faiss.IndexFlatL2(feature_coll.shape[-1])\n",
    "        # kmeans            = faiss.Clustering(feature_coll.shape[-1], n_classes)\n",
    "        # kmeans.niter = 20\n",
    "        # kmeans.min_points_per_centroid = 1\n",
    "        # kmeans.max_points_per_centroid = 1000000000\n",
    "\n",
    "        ### Train Kmeans\n",
    "        # kmeans.train(feature_coll, cpu_cluster_index)\n",
    "        # computed_centroids = faiss.vector_float_to_array(kmeans.centroids).reshape(n_classes, feature_coll.shape[-1])\n",
    "\n",
    "        ### Assign feature points to clusters\n",
    "        # faiss_search_index = faiss.IndexFlatL2(computed_centroids.shape[-1])\n",
    "        # faiss_search_index.add(computed_centroids)\n",
    "        # _, model_generated_cluster_labels = faiss_search_index.search(feature_coll, 1)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_classes, random_state=0).fit(feature_coll)\n",
    "        model_generated_cluster_labels = kmeans.labels_\n",
    "        computed_centroids = kmeans.cluster_centers_\n",
    "\n",
    "        ### Compute NMI\n",
    "        NMI = metrics.cluster.normalized_mutual_info_score(model_generated_cluster_labels.reshape(-1), target_labels.reshape(-1))\n",
    "\n",
    "\n",
    "        ### Recover max(k_vals) nearest neighbours to use for recall computation\n",
    "        # faiss_search_index  = faiss.IndexFlatL2(feature_coll.shape[-1])\n",
    "        # faiss_search_index.add(feature_coll)\n",
    "        # _, k_closest_points = faiss_search_index.search(feature_coll, int(np.max(k_vals)+1))\n",
    "\n",
    "        k_closest_points  = squareform(pdist(feature_coll)).argsort(1)[:, :int(np.max(k_vals)+1)]\n",
    "        k_closest_classes = target_labels.reshape(-1)[k_closest_points[:, 1:]]\n",
    "\n",
    "        ### Compute Recall\n",
    "        recall_all_k = []\n",
    "        for k in k_vals:\n",
    "            recall_at_k = np.sum([1 for target, recalled_predictions in zip(target_labels, k_closest_classes) if target in recalled_predictions[:k]])/len(target_labels)\n",
    "            recall_all_k.append(recall_at_k)\n",
    "\n",
    "        ### Compute F1 Score\n",
    "        F1 = f1_score(model_generated_cluster_labels, target_labels, feature_coll, computed_centroids)\n",
    "\n",
    "    return F1, NMI, recall_all_k, feature_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "image_paths = np.array(dataloaders['testing'].dataset.image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Evaluation Metrics...: 100%|| 541/541 [5:43:09<00:00, 38.06s/it]  \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        #Compute Metrics\n",
    "        F1, NMI, recall_at_ks, feature_matrix_all = eval_metrics_one_dataset(model, dataloaders['testing'], 'cpu', [1,2,4,8])\n",
    "        #Make printable summary string.\n",
    "        result_str = ', '.join('@{0}: {1:.4f}'.format(k,rec) for k,rec in zip(opt.k_vals, recall_at_ks))\n",
    "        result_str = 'Epoch (Test) {0}: NMI [{1:.4f}] | F1 [{2:.4f}] | Recall [{3}]'.format(epoch, NMI, F1, result_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
